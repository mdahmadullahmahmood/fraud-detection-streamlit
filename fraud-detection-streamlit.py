# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17FBqXtL7DNca236zDli1tXhWvQj_sbuk
"""

!pip install tensorflow scikit-learn joblib streamlit email-validator

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
import joblib
import os
import streamlit as st

MODEL_PATH = "/content/model.h5"
SCALER_PATH = "/content/scaler.pkl"

import numpy as np
import pandas as pd
import joblib
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder

def load_data():
    file_path = "/content/synthetic_fraud_dataset.csv"  # Ensure correct path
    data = pd.read_csv(file_path)

    # Define the four selected features
    numerical_features = ["Transaction_Amount", "Account_Balance", "Risk_Score"]
    categorical_features = ["Transaction_Type"]  # Assuming Transaction_Type is categorical

    # Keep only selected features
    data = data[numerical_features + categorical_features]

    # One-hot encode the categorical feature
    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
    categorical_data = encoder.fit_transform(data[categorical_features])

    # Scale the numerical features
    scaler = MinMaxScaler()
    numerical_data = scaler.fit_transform(data[numerical_features])

    # Combine processed numerical and categorical data
    X = np.hstack((numerical_data, categorical_data))

    # Save the scaler and encoder for future use
    joblib.dump(scaler, "scaler.pkl")
    joblib.dump(encoder, "encoder.pkl")

    return X, scaler, encoder

# Run the function
X, scaler, encoder = load_data()

# Check the processed data shape
print("Processed Data Shape:", X.shape)

def build_autoencoder(input_dim):
    input_layer = tf.keras.layers.Input(shape=(input_dim,))
    encoded = tf.keras.layers.Dense(128, activation="relu")(input_layer)
    encoded = tf.keras.layers.Dense(64, activation="relu")(encoded)
    encoded = tf.keras.layers.Dense(32, activation="relu")(encoded)
    latent = tf.keras.layers.Dense(16, activation="relu")(encoded)
    decoded = tf.keras.layers.Dense(32, activation="relu")(latent)
    decoded = tf.keras.layers.Dense(64, activation="relu")(decoded)
    decoded = tf.keras.layers.Dense(128, activation="relu")(decoded)
    output_layer = tf.keras.layers.Dense(input_dim, activation="sigmoid")(decoded)

    autoencoder = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)
    autoencoder.compile(optimizer="adam", loss="mse")
    return autoencoder

def train_and_save_model():
    X, scaler, encoder = load_data()
    X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)
    autoencoder = build_autoencoder(input_dim=X.shape[1])
    autoencoder.fit(X_train, X_train, epochs=50, batch_size=64, shuffle=True, validation_split=0.2)
    autoencoder.save("model.h5")

    X_test_pred = autoencoder.predict(X_test)
    reconstruction_errors = np.mean(np.square(X_test - X_test_pred), axis=1)
    threshold = np.percentile(reconstruction_errors, 95)

    return threshold

threshold = train_and_save_model()

def detect_anomalies(autoencoder, X, threshold):
    X_pred = autoencoder.predict(X)
    reconstruction_error = np.mean(np.square(X - X_pred), axis=1)
    anomalies = reconstruction_error > threshold
    return anomalies, reconstruction_error

def process_transaction(transaction, threshold):
    autoencoder = tf.keras.models.load_model("model.h5")
    scaler = joblib.load("scaler.pkl")
    encoder = joblib.load("encoder.pkl")

    required_numerical_features = ["Transaction_Amount", "Account_Balance", "Risk_Score"]
    required_categorical_features = ["Transaction_Type"]  # Must be categorical

    # âœ… Ensure required features exist in transaction
    missing_features = [feat for feat in (required_numerical_features + required_categorical_features) if feat not in transaction]
    if missing_features:
        raise ValueError(f"Missing required features: {missing_features}")

    # âœ… Convert to DataFrame
    transaction_df = pd.DataFrame([transaction])

    # âœ… Handle categorical transformation properly
    transaction_df["Transaction_Type"] = transaction_df["Transaction_Type"].astype(str)  # Convert to string if needed
    categorical_data = encoder.transform(transaction_df[required_categorical_features])

    # âœ… Scale numerical features
    numerical_data = scaler.transform(transaction_df[required_numerical_features])

    # âœ… Combine scaled numerical & encoded categorical data
    transaction_scaled = np.hstack((numerical_data, categorical_data))

    # âœ… Detect anomalies properly
    is_anomaly, error = detect_anomalies(autoencoder, transaction_scaled, threshold)

    if is_anomaly[0]:  # Ensure it's a single boolean, not an array
        alert_message = f"ðŸš¨ ALERT! Anomalous transaction detected. Reconstruction Error: {error[0]}"
    else:
        alert_message = "âœ… Transaction is normal."

    return alert_message, transaction_df

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import joblib
# import tensorflow as tf
# import numpy as np
# 
# st.title("Real-Time Fraud Detection System")
# 
# amount = st.number_input("Transaction Amount", min_value=0.0, step=0.01)
# transaction_type = st.selectbox("Transaction Type", ["POS", "Online", "ATM Withdrawal", "Bank Transfer"])
# account_balance = st.number_input("Account Balance", min_value=0.0, step=0.01)
# risk_score = st.number_input("Risk Score", min_value=0.0, step=0.01)
# 
# if st.button("Check for Fraud"):
#     transaction = {
#         "Transaction_Amount": amount,
#         "Transaction_Type": transaction_type,
#         "Account_Balance": account_balance,
#         "Risk_Score": risk_score
#     }
#     alert_message, transaction_df = process_transaction(transaction,threshold)
#     st.write(alert_message)
#     st.dataframe(transaction_df)
# else:
#    st.error("Please fill in all fields.")
# 
#

from google.colab import files #used to save the importantant files
files.download("model.h5")
files.download("scaler.pkl")
files.download("encoder.pkl")

!wget -q -O - ipv4.icanhazip.com # it will generate the localtunnel code

!streamlit run app.py & npx localtunnel --port 8501 # this will generate the streamlit website